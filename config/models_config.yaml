# Ollama Models Configuration - Performance Focused
# Uses Ollama API for model management instead of local files

models:
  # Quick filter model (fastest, smallest context) - Phi-2
  quick_filter:
    ollama_model: "phi2"
    context_window: 512  # Reduced for speed
    max_tokens: 32       # Minimal tokens for quick decisions
    temperature: 0.01    # Very low for consistent results
    top_p: 0.99
    
  # Fast classification model (optimized for speed) - Phi-2
  fast_classification:
    ollama_model: "phi2"
    context_window: 1024 # Reduced for speed
    max_tokens: 128      # Reduced for faster responses
    temperature: 0.05    # Low for consistency
    top_p: 0.95
    
  # Detailed analysis model (quality focused) - Llama 3.1 8B
  detailed_analysis:
    ollama_model: "llama3.1:8b"
    context_window: 4096 # Reduced from 8192 for speed
    max_tokens: 512      # Reduced for faster responses
    temperature: 0.1     # Balanced for quality/speed
    top_p: 0.9

# Performance optimization settings
performance:
  # Processing strategy
  enable_quick_filter: true
  enable_early_termination: true
  min_confidence_for_detailed: 0.3
  
  # Caching
  enable_caching: true
  cache_size: 2000       # Increased cache size
  cache_ttl: 7200        # 2 hours cache TTL
  
  # Content limits
  min_content_length: 50
  max_content_length: 2000  # Reduced for speed
  
  # Ollama settings
  ollama_base_url: "http://localhost:11434"
  request_timeout: 30
  max_retries: 3

# Fallback configuration
fallback:
  ollama_model: "phi2"  # Use Phi-2 as fallback
  context_window: 1024
  max_tokens: 128
  temperature: 0.1

# Processing pipeline configuration
pipeline:
  # Quick filter thresholds
  quick_filter:
    min_relevance_score: 0.2
    max_content_length: 1000
    
  # Fast classification thresholds  
  fast_classification:
    min_confidence: 0.5
    max_content_length: 2000
    
  # Detailed analysis triggers
  detailed_analysis:
    min_confidence: 0.7
    min_relevance_score: 0.4
    max_content_length: 3000

# Environment-specific optimizations
environment:
  # macOS specific settings
  macos:
    ollama_base_url: "http://localhost:11434"
    request_timeout: 30
    
  # Linux specific settings
  linux:
    ollama_base_url: "http://localhost:11434"
    request_timeout: 30

# Optimization notes:
# - Uses Ollama API instead of local model files
# - Reduced context windows for faster processing
# - Implemented model hierarchy (Phi-2 -> Llama 3.1 8B)
# - Added early termination logic
# - Increased caching for better performance
# - Reduced content length limits for speed
# - Configurable Ollama base URL and timeouts 