# High-Performance LLM Models Configuration - Optimized for macOS
# Uses CPU-optimized settings to avoid Metal GPU warnings and maximize performance

models:
  # Quick filter model (fastest, smallest context) - Phi-2
  quick_filter:
    path: "models/phi-2.gguf"
    context_window: 1024  # Increased to match model training context
    max_tokens: 64        # Increased for better responses
    temperature: 0.01
    top_p: 0.99
    n_gpu_layers: 0       # CPU only to avoid Metal warnings
    n_threads: 4          # Optimized for macOS
    batch_size: 8
    
  # Fast classification model (optimized for speed) - Phi-2
  fast_classification:
    path: "models/phi-2.gguf"
    context_window: 2048  # Increased to match model training context
    max_tokens: 256       # Increased for better responses
    temperature: 0.05
    top_p: 0.95
    n_gpu_layers: 0       # CPU only to avoid Metal warnings
    n_threads: 6          # Optimized for macOS
    batch_size: 4
    
  # Detailed analysis model (balanced performance/quality) - Llama 3.1 8B
  detailed_analysis:
    path: "models/llama-3.1-8b-instruct-q6_k.gguf"
    context_window: 8192  # Increased to better utilize model capacity
    max_tokens: 1024      # Increased for better responses
    temperature: 0.1
    top_p: 0.9
    n_gpu_layers: 0       # CPU only to avoid Metal warnings
    n_threads: 8          # Optimized for macOS
    batch_size: 2

# Performance optimization settings
performance:
  # Parallel processing
  max_workers: 4
  batch_size: 20
  
  # Caching
  enable_caching: true
  cache_size: 1000
  cache_ttl: 3600  # 1 hour
  
  # Early filtering
  enable_quick_filter: true
  min_content_length: 50
  max_content_length: 3000
  
  # Model optimization
  use_quantized_models: true
  enable_gpu_acceleration: false  # Disabled to avoid Metal warnings
  optimize_memory_usage: true

# Fallback configuration - CPU optimized
fallback:
  model_path: "models/llama-3.1-8b-instruct-q6_k.gguf"
  context_window: 4096
  max_tokens: 512
  temperature: 0.1
  n_gpu_layers: 0  # CPU only
  n_threads: 6

# Environment-specific optimizations
environment:
  # macOS specific settings
  macos:
    use_metal: false      # Disable Metal to avoid warnings
    cpu_threads: 8        # Optimize for macOS CPU
    memory_efficient: true
    
  # Linux specific settings (if needed)
  linux:
    use_cuda: false       # Disable CUDA if not available
    cpu_threads: 8
    memory_efficient: true

# Alternative model suggestions for even better performance
alternative_models:
  # For quick filtering (very fast)
  tiny_filter:
    path: "models/phi-2.gguf"
    context_window: 1024
    max_tokens: 64
    n_gpu_layers: 0
    
  # For classification (fast) - Llama 3.2 3B option
  small_classifier:
    path: "models/llama-3.2-3b-instruct-q6_k.gguf"
    context_window: 2048
    max_tokens: 256
    n_gpu_layers: 0
    
  # For analysis (balanced) - Llama 3.2 7B option
  medium_analyzer:
    path: "models/llama-3.2-7b-instruct-q6_k.gguf"
    context_window: 4096
    max_tokens: 512
    n_gpu_layers: 0

# Optimization notes:
# - Disabled GPU acceleration to avoid Metal warnings
# - Increased context windows to better utilize model capacity
# - Optimized thread counts for macOS
# - Increased max_tokens for better response quality
# - CPU-only mode ensures compatibility and stability
