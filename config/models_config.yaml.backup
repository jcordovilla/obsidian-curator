# High-Performance LLM Models Configuration
models:
  # Quick filter model (fastest, smallest context)
  quick_filter:
    path: "models/llama-3.1-8b-instruct-q6_k.gguf"  # Could use tiny model like Phi-2
    context_window: 1024
    max_tokens: 64
    temperature: 0.01
    top_p: 0.99
    n_gpu_layers: -1
    n_threads: 2
    batch_size: 8  # Process multiple notes at once
    
  # Fast classification model (optimized for speed)
  fast_classification:
    path: "models/llama-3.1-8b-instruct-q6_k.gguf"  # Could use 3B or 7B model
    context_window: 2048
    max_tokens: 256
    temperature: 0.05
    top_p: 0.95
    n_gpu_layers: -1
    n_threads: 4
    batch_size: 4
    
  # Detailed analysis model (balanced performance/quality)
  detailed_analysis:
    path: "models/llama-3.1-8b-instruct-q6_k.gguf"
    context_window: 4096  # Reduced from 8192
    max_tokens: 512       # Reduced from 1024
    temperature: 0.1
    top_p: 0.9
    n_gpu_layers: -1
    n_threads: 8
    batch_size: 2

# Performance optimization settings
performance:
  # Parallel processing
  max_workers: 4
  batch_size: 20
  
  # Caching
  enable_caching: true
  cache_size: 1000
  cache_ttl: 3600  # 1 hour
  
  # Early filtering
  enable_quick_filter: true
  min_content_length: 50
  max_content_length: 3000
  
  # Model optimization
  use_quantized_models: true
  enable_gpu_acceleration: true
  optimize_memory_usage: true

# Fallback configuration
fallback:
  model_path: "models/llama-3.1-8b-instruct-q6_k.gguf"
  context_window: 2048
  max_tokens: 256
  temperature: 0.1
  n_gpu_layers: 0  # CPU only as fallback
  n_threads: 4

# Alternative model suggestions for even better performance
alternative_models:
  # For quick filtering (very fast)
  tiny_filter:
    path: "models/phi-2.gguf"  # 2.7B parameters
    context_window: 512
    max_tokens: 32
    
  # For classification (fast)
  small_classifier:
    path: "models/llama-3.1-3b-instruct-q6_k.gguf"  # 3B parameters
    context_window: 1024
    max_tokens: 128
    
  # For analysis (balanced)
  medium_analyzer:
    path: "models/llama-3.1-7b-instruct-q6_k.gguf"  # 7B parameters
    context_window: 2048
    max_tokens: 256 